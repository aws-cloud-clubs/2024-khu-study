# #9 웹 크롤러 설계

- 검색 엔진 인덱싱 (검색 엔질을 위한 인덱싱)
- 웹 아카이빙 (추후 사용을 위해 장기보관)
- 웹 마이닝 (유용한 지식 도출)
- 웹 모니터링 (상표권 침해사례 모니터링)

## 1단계 문제 이해 및 설계 범위 특정

> 웹 크롤러 알고리즘

1.  URL 집합 입력 -> 해당 URL이 가르키는 모든 웹 다운로드
2.  다운받은 페이지에서 URL 추출
3.  추출된 URL도 다운로드 이 과정을 계속 반복

**확인할 사항**

- 크롤러의 용도는 무엇인가?
- 매달 얼마나 많은 페이지를 수집해야 하는가?
- 새로운 페이지나 업데이트된 페이지도 고려하는가?
- 수집한 페이지는 저장하는가?
- 중복된 컨텐츠 처리는?

**주의할 속성**

- 규모 확장성
- 안정성
- 예절
- 확장성

## 2단계 개략적 설계안 제시 및 동의 구하기

![](https://velog.velcdn.com/images/kyy00n/post/7b970245-ec85-4421-b3f1-d4809034737d/image.png)

- 시작 URL 집합
- 미수집 URL 저장소
- HTML 다운로더
- 도메인 이름 변환기
- 콘텐츠 파서
- 중복 콘텐츠인가?
- URL 추출기
- URL필터
- 이미 방문한 URL?
- URL저장소

## 3단계 상세 설계

> DFS? BFS?

DFS는 어느 정도로 깊숙이 가게 될지 가늠하기 어려워 피하는 것이 좋다.

따라서 BFS를 주로 사용한다.
BUT...

- 한 페이지에서 나오는 링크는 보통 같은 서버 -> 예의 없는 크롤러
- URL간 우선순위 X BUT 페이지 마다 품질 차이가 존재함.

> 미수집 URL 저장소

위 문제를 해결하기 위한 방법.

- 동일 서버로 너무 많은 요청을 보내는 것을 삼가한다. \*큐 라우터, 매핑테이블, FIFO, 큐 선택기, 작업 스레드

- 유용성에 따라 우선순위를 나눈다. \*순위결정장치

- 웹 페이지 변경 이력에 따라 재수집한다.

> HTML다운로더

**성능 최적화를 위해...**

- 분산 크롤링
- 도메인 이름 변환 결과 캐시
- 지역성
- 짧은 타임아웃

**안정성**

- 안정 해시
- 크롤링 상태 및 수집 데이터 저장
- 예외 처리
- 데이터 검증

## 4단계 마무리

추가로 논의할만한 사항

- 서버 측 렌더링
- 원치 않는 페이지 필터링
- 데이터베이스 다중화 및 샤딩
- 수평적 규모 확장성
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션
