# 웹 크롤러 설계

웹 크롤러는 '로봇(robot)' 또는 '스파이더(spider)'라고 불리며, 웹에 새로 올라온 컨텐츠를 찾아내고 갱신하는것이 목적이다. 크롤러의 이용 목적은 다음과 같다.

- 검색 엔진 인덱싱(search engine indexing): 웹 페이지를 모아 검색 엔진을 위한 로컬 인텍스를 만든다. 대표적으로 구글봇(Googlebot)이 있다.
- 웹 아카이빙(web archiving): 장기보관 목적으로 정보를 모은다.
- 웹 마이닝(web mining): 인터넷의 유용한 정보들을 도출해낸다. 금융 기업들은 크롤러를 이용해 주주총회 자료나 연차 보고서등을 다운받는다.
- 웹 모니터링(web monitoring): 인터넷에서 저작권이나 상표가 침해되는 사례를 모니터링한다.


## 1. 문제 이해 및 설계 범위 확정

- 웹 크롤러의 기본 알고리즘
  - URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.
  - 다운받은 웹 페이지에서 URL들을 추출한다.
  - 추출된 URL들을 다운로드할 URL 목록에 추가하고 위 과정을 반복한다.
- 요구사항
  -  검색 엔진 인덱싱에 쓰일 목적
  -  매달 10억 개의 웹 페이지를 수집해야 함
  -  수집한 웹페이지는 5년간 저장
  -  중복 컨텐츠는 무시
-  웹 크롤러의 속성
  - 규모 확장성: 병행성(parallelism)을 이용한 효과적인 웹 크롤링 필요
  - 안전성(robustness): 잘못 작성된 HTML, 반응 없는 서버, 장애, 악성 코드 등 비정상적인 입력에 대응할 수 있는 능력
  - 예절(politeness): 웹사이트에 짧은 시간에 너무 많은 요청을 보내면 안됨
  - 확장성(extensibility): 새로운 콘텐츠 지원이 쉬워야 함

### 개략적 규모 추정

- QPS(Queries Per Seceond): 10억 / 30일 / 24시간 / 3600초 = 약 400QPS
- Peak QPS: 2 * QPS = 800QPS
- 웹 페이지의 평균 크기는 500kb
- 저장 용량: 10억 * 500kb = 500TB/월 -> 5년 보관시 30PB

## 2. 계략적 설계안 제시 및 동의 구하기

<img width="855" alt="스크린샷 2024-07-15 오전 4 24 27" src="https://github.com/user-attachments/assets/712316a8-6027-4e59-ab93-4e3243d5cb8f">

- 시작 URL 집합은 상황에 맞게 고르자
- 미수집 URL 저장소(URL frontier): 아직 다운로드되지 않은 URL을 저장하는 저장소. FIFO 구조이다.
- HTML 다운로더: HTML 파일을 다운로드한다.
- 도메인 이름 변환기: URL을 IP 주소로 변환해준다.
- 콘텐츠 파서: 다운로드한 웹 페이지를 파싱하고 검증한다.
- 중복 컨텐츠 거르기: 웹에 공개된 29%의 웹 페이지는 중복이라고 한다. 중복인 컨텐츠를 거르기 위해 해시값 비교를 사용할 수 있다.
- 콘텐츠 저장소: 인기 있는 콘텐츠는 메모리에 두고 그 외의 콘텐츠는 디스크에 저장한다.
- URL 추출기: HTML에서 URL을 추출한다. 상대 경로를 절대 경로로 변환한다.
- URL 필터: 파일 확장자를 가졌거나 제외 목록에 등록된 URL등을 필터링한다.
- 이미 방문한 URL?: 블룸 필터 등을 활용해 이미 방문한 URL을 다시 방문하지 않도록 한다.
- URL 저장소: 지금까지 방문한 URL을 저장한다.

## 3. 상세 설계

- 웹은 유향 그래프와 같아서 DFS, BFS와 같은 그래프 탐색 알고리즘을 적용할 수 있다. 하지만 웹은 깊이가 얼마나 깊을지 가늠하기 힘듦으로 DFS는 좋은 선택이 아니다.
- BFS는 같은 웹사이트에 계속 요청을 보내게 된다. 이런 크롤러를 '예의 없는(impolite)' 크롤러라고 한다.
- 표준 BFS는 URL간 우선순위를 두지 않는다. 하지만 모든 URL이 동일하게 중요한 것이 아니므로 페이지 순위(page rank), 사용자 트래픽 양, 업데이트 빈도 등 여러가지 척도를 두어 우선순위를 구별하는 것이 좋다.
- 같은 웹사이트에 병렬적으로 요청을 보내게 될 경우 해당 서버가 DoS 공격으로 오해할 소지가 있다. 따라서 호스트마다 큐를 따로 두어 분산 처리 환경에서도 한 호스트에 과도하게 요청이 집중되는 것을 막을 수 있다.
<img width="855" alt="스크린샷 2024-07-15 오전 4 45 10" src="https://github.com/user-attachments/assets/f1fc7fa5-c84e-479c-ae2b-ab8eddaa249d">

- 우선순위가 높은 URL을 더 많이 크롤링하기 위해 우선순위별 큐를 둔다.
<img width="742" alt="스크린샷 2024-07-15 오전 4 50 13" src="https://github.com/user-attachments/assets/3bd948ad-70e5-4b21-be60-fe5dbf0732d6">

- 웹 페이지는 수시로 추가되고, 삭제되고, 변경되므로 이미 다운로드한 페이지도 주기적으로 재수집할 필요가 있다.
- 웹 페이지 변경 이력을 활용하거나, 우선순위가 높은 페이지를 좀더 자주 재수집할 수 있다.
- Robots.txt는 웹사이트가 크롤러에게 수집해도 되는 페이지 목록을 제공하는 파일이다.
- 크롤러 성능 최적화에는 다음 방법들이 사용된다.
  - 분산 크롤링: 여러 쓰레드를 이용해 분산 작업을 처리한다.
  - 도메인 이름 변환 결과 캐시: DNS 요청은 동기적으로 이뤄지므로 하나의 스레드에서 DNS에 요청중일 때 다른 스레드의 DNS 요청은 전부 블록된다. 따라서 DNS 요청의 결과를 캐싱해야 한다.
  - 크롤링 서버별로 지역적으로 가까운 서버의 페이지를 다운로드하도록 한다.
  - 타임아웃을 짧게 두어 응답하지 않는 페이지를 빠르게 건너뛴다.
- 안전성을 향상시키기 위해서는 다음 방법들을 활용 가능하다.
  - 다운로더 서버 부하를 분산하기 쉽도록 안정 해시(consistent hashing)을 이용한다.
  - 장애 발생 후 복구할 수 있도록 크롤링 상태를 따로 저장해두었다가 재시작할 수 있도록 한다.
  - 예외처리, 데이터 검증
- 새로운 모듈을 끼워넣음으로써 기능 확장을 할 수 있다.
<img width="1052" alt="스크린샷 2024-07-15 오전 5 33 05" src="https://github.com/user-attachments/assets/4431dc93-cff9-4aec-9d6f-96c64f1460cb">

- 문제가 있는 콘텐츠를 회피하는 방법은 다음과 같다.
  - 해시나 체크섬을 사용해 중복 컨텐츠를 탐지한다.
  - 거미 덫: 무한히 깊은 디렉터리 구조를 가지는 링크이다. URL 최대 길이를 설정함으로써 해결 가능하다.
  - 광고, 스크립트 코드, 스팸 URL 같은 것들은 크롤링할 필요가 없으니 차단한다.
 
## 4. 마무리

- 동적 웹 컨텐츠를 파싱하기 위해 서버사이드 렌더링을 적용한다.
- 스팸 방지 컴포넌트를 두어 원치 않는 페이지 필터링
- 데이터베이스 다중화 및 샤딩
- 수평적 규모 확장성: 크롤러의 규모가 커질 때를 대비하여 수평적 규모 확장이 쉽도록 서버를 무상태로 만드는 것이 중요하다.
- 가용성, 일관성, 안정성 확보
- 데이터 수집 및 분석 솔루션
