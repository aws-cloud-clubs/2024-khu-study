# #9 웹 크롤러 설계
- 웹 크롤러는 로봇(robot) 또는 스파이더(spider)라고도 부름

웹 크롤러는 몇 개 웹 페이지에서 시작하여 그 링크를 따라 나가면서 새로운 콘텐츠를 수집한다.

![웹 크롤러](https://github.com/user-attachments/assets/4d0eb3ef-c1a2-4ec8-bb7e-7fab455f650b)

크롤러는 다양하게 이용된다.

- 검색 엔진 인덱싱(search engine indexing)
    - 크롤러의 가장 보편적인 용례
    - 크롤러는 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스(local index)를 만듬
    - 일례로, Googlebot은 구글(Google) 검색 엔진이 사용하는 웹 크롤러
- 웹 아카이빙(web archiving)
    - 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
    - 많은 국립 도서관이 크롤러를 돌려 웹 사이트를 아카이빙함
    - 대표적으로는 미국 국회 도서관(US Library of Congress), EU 웹 아카이브가 존재
- 웹 마이닝(web mining)
    - 웹의 폭발적 성장세는 데이터 마이닝(data mining) 업계에 전례 없는 기회
    - 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출해 낼 수 있는 것
    - 일례로, 유명 금융 기업들은 크롤러를 사용해 주주총회 자료나 연차 보고서(annual report)를 다운받아 기업의 핵심 사업 방향을 알아냄
- 웹 모니터링(web monitoring)
    - 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링 가능
    - 일례로, 디지마크(Digimarc)사는 웹 크롤러를 사용해 해적판 저작물을 찾아내서 보고함

**웹 크롤러의 기본 알고리즘**

1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드
2. 다운받은 웹 페이지에서 URL들을 추출
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복  

하지만, 웹 크롤러가 정말로 이처럼 단순하게 동작하지 않는다. 엄청난 규모 확장성을 갖는 웹 크롤러를 설계하는 것은 엄청나게 어려운 작업이다.

#### 좋은 웹 크롤러가 만족시켜야 할 다음과 같은 속성에 주의를 기울이는 것도 좋다.

- 규모 확장성
    - 오늘날 웹에는 수십억 개의 페이지가 존재하며, 웹은 거대함
    - 병행성(parallelism)을 활용하면 보다 효과적으로 웹 크롤링이 가능
- 안정성(robustness)
    - 잘못 작성된 HTML, 아무 반응이 없는 서버, 장애, 악성 코드가 붙어 있는 링크 등 웹은 함정으로 가득함
    - 크롤러는 이런 비정상적 입력이나 환경에 잘 대응해야 함
- 예절(politeness)
    - 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내면 안됨
- 확장성(extensibility)
    - 새로운 형태의 콘텐츠를 지원하기 쉬워야 함
    - 이미지 파일도 크롤링하고 싶다고 예를 들면, 이를 위해 전체 시스템을 새로 설계해야 한다면 곤란함


### 시작 URL 집합
- 시작 URL 집합은 웹 크롤러가 크롤링을 시작하는 출발점

일반적으로는 **전체 URL 공간을 작은 부분집합으로 나누는 전략**을 쓴다. 나라별로 인기 있는 웹 사이트가 다르다는 점에 착안하는 것이다. 또 다른 방법은 **주제별로 다른 시작 URL을 사용하는 것**이다. 예시로, URL 공간을 쇼핑, 스포츠, 건강 등의 주제별로 세분화하고 그 각각에 다른 시작 URL을 쓰는 것이다.

> 시작 URL로 무엇을 쓸 것이냐는 질문에 정답은 없으므로 의도가 무엇인지만 정확히 전달하자.

### 미수집 URL 저장소

대부분의 현대적 웹 크롤러는 크롤링 상태를 두 가지로 나눠 관리한다.

저장 관리하는 컴포넌트를 미수집 URL 저장소(URL frontier)라고 부른다. FIFO(First-In-First-Out) 큐(queue)라고 생각하면 된다.

1. 다운로드할 URL
2. 다운로드된 URL

### HTML 다운로더(downloader)

인터넷에서 웹 페이지를 다운로드하는 컴포넌트이다. 다운로드할 페이지의 URL은 미수집 URL 저장소가 제공한다.

### 도메인 이름 변환기

웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요하다. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL에 대응되는 IP 주소를 알아낸다.

### 콘텐츠 파서

웹 페이지를 다운로드하면 파싱(parsing)과 검증(validation) 절차를 거쳐야 한다. 이상한 웹 페이지는 문제를 일으킬 수 있는데다 저장 공간만 낭비하게 되기 때문이다.
크롤링 서버 안에 콘텐츠 파서를 구현하면 크롤링 과정이 느려지게 될 수 있으므로, 독립된 컴포넌트로 만든다.

### 중복 콘테츠인가?

> 웹에 공개된 연구 결과에 따르면, 29% 가량의 웹 페이지 콘텐츠는 중복이다. 따라서 같은 콘텐츠를 여러 번 저장하게 될 수 있다.


본 설계안의 경우, 이 문제를 해결하기 위한 자료 구조를 도입하여 데이터 중복을 줄이고 데이터 처리에 소요되는 시간을 줄인다. 이미 시스템에 저장된 콘텐츠임을 알아내기 쉽게 하는 것이다.

비교 대상의 문서의 수가 많아질수록 두 HTML 문서를 비교하는 가장 간단한 방법은 느리고 비효율적이다. 효과적인 방법은 웹 페이지의 해시 값을 비교하는 것이다.

### 콘텐츠 저장소

콘텐츠 저장소는 HTML 문서를 보관하는 시스템이다.  
저장소를 구현하는 데 쓰일 기술을 고를 때는 저장할 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려해야 한다. 본 설계안의 경우에는 디스크와 메모리를 동시에 사용하는 저장소를 택할 것이다.

- 데이터 양이 너무 많으므로 대부분의 콘텐츠는 디스크에 저장
- 인기 있는 콘텐츠는 메모리에 두어 접근 지연시간을 줄임

### URL 추출기

HTML 페이지를 파싱하여 링크들을 골라내는 역할을 한다.
- 상대 경로(relative path)는 전부  절대 경로(absolute path)로 변환

### URL 필터

특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속시 오류가 발생하는 URL, 접근 제외 목록(deny list)에 포함된 URL 등을 크롤링 대상에서 배제하는 역할

### 이미 방문한 URL?

이 단계를 구현하기 위해서 이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL을 추적할 수 있도록 하는 자료 구조를 사용할 것이다. 이미 방문한 적이 있는 URL인지 추적하면 같은 URL을 여러 번 처리하는 일을 방지할 수 있으므로 서버 부하를 줄이고 시스템이 무한 루프에 빠지는 일을 방지한다.

해당 자료 구조는 블룸 필터(bloom filter)나 해시 테이블이 널리 사용된다.

### URL 저장소

이미 방문한 URL을 보관하는 저장소이다.

### 웹 크롤러 작업 흐름

![웹 크롤러 작업 흐름](https://github.com/user-attachments/assets/00a6f7ff-069d-4cc1-bb08-7498c037b5a9)

① - 시작 URL들을 미수집 URL 저장소에 저장

② - HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져옴

③ - HTML 다운로더는 도메인 이름 변환기를 사용하여 URL의 IP 주소를 알아내고, 해당 IP 주소로 접속하여 웹 페이지를 다운받음

④ - 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증

⑤ - 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠인지 확인하는 절차를 개시

⑥ - 중복 콘텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 확인

- 이미 저장소에 있는 콘텐츠인 경우, 처리하지 않고 버림
- 저장소에 없는 콘텐츠인 경우, 저장소에 저장한 뒤 URL 추출기로 전달

⑦ - URL 추출기는 해당 HTML 페이지에서 링크를 골라냄

⑧ - 골라낸 링크를 URL 필터로 전달

⑨ - 필터링이 끝나고 남은 URL만 중복 URL 판별 단계로 전달

⑩ - 이미 처리한 URL인지 확인하기 위하여, URL 저장소에 보관된 URL인지 확인하고, 이미 저장소에 있는 URL은 버림

⑪ - 저장소에 없는 URL은 URL 저장소에 저장할 뿐 아니라 미수집 URL 저장소에도 전달


## **좋은 크롤러가 갖추어야 하는 특성**

1. 규모 확장성(scalability)
2. 예의
3. 확장성(extensibility)
4. 안정성 등

시간이 허락한다면 면접관과 다음과 같은 것을 추가로 논의하면 좋다.

- 서버 측 렌더링(server-side rendering)
    - 많은 웹 사이트가 자바스크립트, AJAX 등의 기술을 사용해서 링크를 즉석에서 만듬
    - 웹 페이지를 그냥 있는 그대로 다운받아서 파싱해보면 그렇게 동적으로 생성되는 링크는 발견 불가
    - 페이지를 파싱하기 전에 서버 측 렌더링(동적 렌더링dynamic rendering)을 적용하면 해결 가능
- 원치 않는 페이지 필터링
    - 저장 공간 등 크롤링에 소요되는 자원은 유한
    - 스팸 방지(anti-spam) 컴포넌트를 두어 품질이 조악하거나 스팸성인 페이지를 걸러라
- 데이터베이스 다중화 및 샤딩
    - 다중화(replication)나 샤딩(sharding) 같은 기법을 적용하면 데이터 계층의 가용성, 규모 확장성, 안정성 향상
- 수평적 규모 확장성(horizontal scalability)
    - 대규모의 크롤링을 위해서는 다운로드를 실행할 서버가 수백 혹은 수천 대가 필요
    - 수평적 규모 확장성을 달성하는 데 중요한 것은 서버가 상태 정보를 유지하지 않도록 하는 무상태(stateless) 서버로 만드는 것
- 가용성, 일관성, 안정성
    - 성공적인 대형 시스템을 만들기 위해 필수적으로 고려해야 하는 것들
    - 1장에서 자세히 다뤘으니 다시 공부
- 데이터 분석 솔루션(analytics)
    - 데이터를 수집하고 분석하는 것은 어느 시스템에게나 중요
    - 시스템을 세밀히 조정하기 위해서는 이런 데이터와 그 분석 결과가 필수적