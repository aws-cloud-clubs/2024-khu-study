# #6 키-값 저장소 설계

- 키-값 데이터베이스라고도 불리는 비 관계형 데이터베이스.
- 고유 식별자를 키로 갖으며 키는 일반 텍스트, 해시 값일 수도 있음.
- 성능을 위해 짧을수록 좋음.

예시로는 레디스, 아마존 다이나모 등

> 설계할 저장소의 연산

- put(key, value) : 키-값 쌍을 저장소에 저장
- get(ket) : 주어진 키에 매달린 값을 꺼냄.

## 문제 이해 및 설계 범위 확정

- 키-값 쌍의 크기는 10KB 이하
- 큰 데이터를 저장할 수 있어야 함.
- 높은 가용성. -> 장애가 있더라도 빨리 응답
- 데이터 일관성 수준 조정 가능
- 응답 지연시간이 짧음.

## 단일 서버 키-값 저장소

> 한 대의 서버만 사용하여 설계하는 것.

- 가장 단순한 방법은 키-값 모두 메모리에 저장하기 but 사실상 불가능

> 개선 방안

- 데이터 압축
- 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크에 저장

\*물론 그럼에도 한 대 서버로는 부족한 때가 찾아옴. 많은 데이터를 위해서는 분산 키-값 저장소가 필수!

## 분산 키-값 저장소

키-값 저장소를 여러 서버에 분산 저장한다.

> 분산 시스템을 설계하려면 CAP 정리(Consistency, Availability, Partition Tolerance theorem)를 이해하고 있어야 함.

### CAP 정리

데이터 일관성, 가용성, 파티션 감내라는 세 가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 건 불가능하다는 정리.

- 데이터 일관성 : 접속하는 서버에 관계 없이 언제나 같은 데이터를 본다.
- 가용성: 일부 노드에 장애가 발생하더라도 항상 같은 응답을 받는다.
- 파티션 감내 : 파티션은 두 노트 사이에 통신 장애가 발생하였음을 의미. 파티션 감내는 네트워크에 파티션이 생기더라도 시스템은 계속 동작한다는 것을 의미.

그림과 같이 어떤 두 가지를 충족하려면 나머지 하나는 반드시 희생되어야 함.
![](https://azderica.github.io/assets/static/CapImage.07cc2b7.dd8b935c30da4454e015c7f8d2451c9c.png)

키-값 저장소가 어느 두 가지를 만족하느냐에 따라 다음과 같이 분류 됨.

- CP 시스템 : 알관성과 파티션 감내를 지원하는 키-값 저장소 가용성을 희생.
- AP 시스템 : 가용성과 파티션 감내를 지원하는 키-값 저장소. 데이터 일관성 희생.
- CA 시스템 : 일관성과 가용성을 지원하는 키-값 저장소. 파티션 감내는 지원X.
  **BUT**네트워크 장애를 피하는건 불가능하므로 CA 시스템은 실존할 수 없다.

### 이해를 위한 예시

복제 노드 n1, n2, n3에 데이터를 복제하여 보관하는 상황

> 이상적인 상태

네트워크가 파티션되는 상황이 없으므로 n1의 데이터가 n2,n3에 자동적으로 복제되며 데이터 일과성, 가용성 만족
![](https://jonghoonpark.com/assets/images/2023-06-01-key-value-store/image2.png)

**BUT** 실세계에서 파티션 문제는 피할 수 없음.
파티션 문제가 발생하면 일관성과 가용성 사이에서 하나를 선택해야 함.

![](https://jonghoonpark.com/assets/images/2023-06-01-key-value-store/image3.png)

- n3에 기록된 데이터는 n1, n2에 전달X
- n1, n2에 기록한 데이터는 n3 전달X
  -> 데이터 불일치 문제 발생

> 가용성 대신 일관성을 선택(CP시스템)

데이터간 불일치 문제를 피하기 위해 n1과 n2의 쓰기 연산을 중단
**then** 가용성이 깨짐

은행과 같은 시스템이라면 일관성을 반드시 선택할 것임.
이러한 경우에는 오류를 반환.

> 일관성 대신 가용성을 선택(AP시스템)

최신 데이터가 아니더라도 반환해야하는 위험이 있더라도 계속 연산을 수행함. 이후 파티션 문제가 해결되면 n3와 동기화

면접이라면 CAP 정리를 적용하기위해 면접관과 상의해야 함.

### 시스템 컴포넌트 (핵심 컴포넌트와 기술 설명)

> 데이터 파티션

대규모 어플리케이션의 경우 전체 데이터를 한 서버에 넣기란 불가능.
가장 쉬운 해결책은 작은 파티션으로 분할하여 여러 서버에 저장하는 것.

**이때 고려해야할 사항**

- 데이터를 여러 서버에 고르게 분산가능한가
- 노드 추가/삭제시 데이터 이동을 최소화할 수 있는가

**5장의 안정해시**
시계방향으로 순회를 하다 만나는 서버에 저장
*규모 확장 자동화: 시스템 부하에 따라 서버가 자동으로 추가/삭제
*다양성: 각 서버의 용량에 맞게 가상 노드 수 조정 가능

> 데이터 다중화

높은 가용성과 안정성을 위해 데이터를 n개의 서버에 다중화한다.
(n은 튜닝가능)

이때 가상 노드를 사용한다면 물리서버의 개수가 n보다 작아져 중복이 되지 않도록 주의.

같은 데이터 센터의 노드는 데이터 센터의 문제(정전, 네트워크 등)에 의해 동시에 오류가 생길 수 있으므로 데이터의 사본은 다른 센터에 두고 센터간 고속 네트워크로 연결

> 데이터 일관성

다중화된 데이터는 일관성을 '적절히' 유지해야 함.
정족수 합의(Quorum Consensus)프로토콜을 통해 읽기/쓰기 일관성을 보장. \*정족수 : 여러 사람의 합의로 운영되는 의사기관에서 의결을 하는데 필요한 최소한의 참석자 수

N: 사본 개수
W: 쓰기 연산에 대한 정족수. 쓰기 연산이 성공했다 간주하려면 W개의 서버로 부터 성공했다는 응답을 받아야 함.
R: 읽기 연산에 대한 정족수. 읽기 연산이 성공했다 간주하려면 R개의 서버로 부터 성공했다는 응답을 받아야 함.

W, R개에만 저장된다는 것이 아닌 W, R개를 넘어서는 순간부터 다른 서버의 응답을 기다리진 않는 다는 의미.

W, R, N의 값을 정하는 것은 응답 지연과 데이터 일관성 사이의 타협점을 찾는 전형적인 과정

**N,W,R 값에 따른 구성**
R = 1, W = N: 빠른 읽기 연산에 최적화된 시스템
W = 1, R = N: 빠른 쓰기 연산에 최적화된 시스템
W + R > N: 강한 일관성이 보장됨 (보통 N=3, W=R=2)
W + R ≤ N: 강한 일관성이 보장되지 않음

> > 일관성 모델

데이터 일관성의 수준을 결정하는 모델

- 강한 일관성: 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환한다. 절대 이전의 데이터를 볼 수 없다.

- 약한 일관성 : 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못할 수 있다.

- 결과적 일관성 : 약한 일관성의 한 형태. 갱신 결과가 결국 모든 사본에 반영.

강한 일관성은 반영이 될 때까지 모든 읽기/쓰기 금지. 고가용성 시스템에는 부적합.

결과적 일관성의 경우 쓰기 연산이 병렬적으로 발생하면 시스템의 값과 일관성이 깨질 수 있지만 클라이언트가 이를 해결.

> > 비 일관성 해소 기법: 데이터 버저닝

데이터를 다중화하면 가용성은 높아지지만 일관성이 깨질 가능성도 커진다.
버저닝을 통해 데이터가 변겨오딜 때마다 새로운 버전을 만들고 이때 각 버전의 데이터는 변경 불가능.

서버 1, 2가 있고 두 서버가 같은 키 값에 대해 다른 버전의 데이터를 가지고 있다면, 이 둘의 충돌을 어떻게 해결해야할까?

**이때는 벡터 시계를 이용한다.** \*벡터 시계: [서버, 버전]의 순서쌍을 데이터에 메단것. 어떤 버전이 선행이고 후행인지 알 수 있음.
![](https://daeakin.github.io//images/large-system/versioning-3.png)

기록 시 다음 중 하나를 수행

- [Si, vi]가 있으면 vi를 증가시킨다.
- 그렇지 않으면 새 항목 [Si, 1]를 만든다.

1. 클라이언트가 데이터 D1을 시스템에 기록한다. 이 쓰기 연산을 처리한 서버는 Sx이다. 벡터 시계는 D1([Sx, 1])

2. 다른 클라이언트가 데이터 D1을 읽고 D2로 업데이트한 다음 기록한다. D2는 D1에 대한 변경이므로 D1을 덮어쓴다. 이 때 쓰기 연산은 같은 서버 Sx가 처리한다고 가정하자. 벡터 시계는 D2([Sx, 2])로 바뀔 것이다.

3. 다른 클라이언트가 D2를 읽어 D3로 갱신한 다음 기록한다. 이 쓰기 요청은 Sy가 처리한다고 가정하면 벡터시계는 D3([Sx, 2], [Sy, 1])로 바뀐다.

4. Sy가 요청을 처리하기 전에 또 다른 클라이언트가 D2를 읽고 D4로 갱신한 다음 기록한다. 이때 쓰기연산은 서버 Sz가 처리한다고 가정하자. 그러면 벡터시계는 D4([Sx, 2], [Sz, 1])이 된다.

5. 어떤 클라이언트가 D3와 D4를 읽으면 데이터 간 충돌이 있다는 것을 알게된다. 이를 클라이언트가 해소한 후에 서버에 기록한다.

벡터 시계의 단점.

- 총돌 감지 및 해소 로직이 클라이언트에 있으므로 클라이언트 구현이 복잡
- [서버:버전] 순서쌍 개수가 굉장히 빠르게 늘어남. 따라서 임계치를 넘으면 오래된 쌍을 제거해야하는데 그러면 선후 관계가 정확하게 결정되기 어려움. **but** 실제로 이것으로 인해 문제가 발견된 적은 없음

> > 장애 처리

- 대규모 시스템에서 장애는 흔하게 벌어진다.

> > 장애 감지
> > 서버가 죽었다고 바로 해당 서버 장애처리를 하지는 않음. 보통 두 대 이상의 서버가 같은 보고를 해야 장애가 발생했다고 간주.

멀티 캐스팅 채널이 가장 간단한 방법이지만 서버가 많다면 비효율적.
가십 프로토콜과 같은 분산형 장애 감지 솔루션이 효율적임.

가십 프로토콜: 주기적으로 자신의 박동 카운터를 늘리고 목록을 보냄. 갱신이 되지 않는 멤버는 장애 상태로 간주.

> > 일시적 장애처리

가십 프로토콜로 장애를 감지한 시스템은 가용성을 위해 조치를 취함.

- 엄격한 정족수 : 읽기/쓰기 금지
- 느슨한 정족수 : 가용성을 늘리기 위해 건강한 서버를 R, W개 고름
  그리고 요청이 온다면 일시적으로 다른 서버가 받았다가 다시 처리후에 해당 서버로 인계.(단서 후 임시 위탁기법)

> > 영구 장애처리

반-엔트로피 프로토콜 구현.

- 키 공간을 버킷으로 나눔
- 버킷에 포함된 키에 균등분포 해시함수 적용
- 버킷별 해시 값을 레이블로 갖는 노드를 만듦
- 자식 노드의 레이블로부터 새로운 해시 값을 계산하여 이진트리 구성

> > 데이터 센터 장애 처리

정전, 네트워크 장애, 자연재해 등의 이유로 발생
데이터를 여러 데이터센터에 다중화 하는 것이 중요함.

> 시스템 아키텍처 다이어그램

- 클라이언트는 키-값 저장소가 제공하는 API와 통신
- 중재자는 클라이언트에게 키-값 저장소에 대한 프록시역할
- 노드는 안정 해시와 해시 링 위에 분포
- 노드를 자동으로 추가/삭제할 수 있도록 완전히 분산
- 데이터는 여러 노드에 다중화
- 모든 노드가 같은 책임 -> SPOF는 존재하지 않음
  ![](https://blog.kakaocdn.net/dn/yn1Ff/btroyHiuCrJ/eKIlLAwCXGkIk55DZ2hbw1/img.png)

> 쓰기경로

1. 쓰기 요청이 커밋 로그 파일에 기록
2. 데이터가 메모리 캐시에 기록
3. 임계치에 도달한다면 SSTable에 기록
   \*SSTable: Sorted-String Table

> 읽기경로

1. 데이터가 메모리 캐시에 있는지 확인. 있다면 바로 반환
2. 없다면 디스크에서 가져옴. 블룸필터를 검사.
3. 어떤 SSTable에 키가 보관되었는지 확인
4. SSTable 에서 값을 가져옴.
